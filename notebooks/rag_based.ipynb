{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Reverse Recommendation and RAG for Anomaly Detection\n",
    "Now that we've already seen how to build the reverse recommendation system based on the similarity scores, let's see how we can combine it with the RAG model to not only detect the anomalies but also explain why they are anomalies.\n",
    "\n",
    "We keep everything the same as the previous reverse recommendation system approach, loading the data, getting random users, converting transactional data to description, embedding the descriptions, storing these embeddings in the qdrant database. The whole workflow till storing the embeddings in the qdrant database is the same except that we are also loading the customer basic information and registered address information which will be used in the context of the RAG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import (\n",
    "    convert_transaction_data_to_str,\n",
    "    get_user_basic_info,\n",
    "    get_transactional_data,\n",
    "    embed_transaction,\n",
    "    insert_transaction,\n",
    "    get_context_for_anomaly_detection,\n",
    "    load_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/quamer23nasim38/reverse-recommendation-for-anomaly-detection/'\n",
    "data_path = 'data/fraudTrain.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quamer23nasim38/reverse-recommendation-for-anomaly-detection/notebooks/../utils.py:268: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['age'] = data['dob'].map(dob_to_age)\n",
      "/home/quamer23nasim38/reverse-recommendation-for-anomaly-detection/notebooks/../utils.py:271: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.gender = data.gender.replace({\n",
      "/home/quamer23nasim38/reverse-recommendation-for-anomaly-detection/notebooks/../utils.py:277: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['name'] = data['first'] + ' ' + data['last']\n",
      "/home/quamer23nasim38/reverse-recommendation-for-anomaly-detection/notebooks/../utils.py:280: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['merchant'] = data.merchant.str.replace('fraud_', '')\n"
     ]
    }
   ],
   "source": [
    "data, random_cc_num = load_data(root, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load the embedding model that will be used later to convert the transaction data into vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load the pre-trained model\n",
    "embedding_model_id = \"BAAI/bge-small-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(embedding_model_id)\n",
    "model = AutoModel.from_pretrained(embedding_model_id)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have loaded the data and embedding model. Now in RAG based approach, we extract the basic customer informations (Name, Age, Gender, and Job) and the registered address of the customer. All these information about customer doesn't change with each transaction and hence we can store them separately without converting them into embeddings. We will use these information the RAG based approach to help understand the LLM in detecting the fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Information Loaded Successfully for 630424987505\n"
     ]
    }
   ],
   "source": [
    "for user in random_cc_num:\n",
    "    # Get the data for the user\n",
    "    user_data = data[data['cc_num'] == user]\n",
    "    # Filter out the fraud transactions\n",
    "    user_data = user_data[user_data['is_fraud'] == 0]\n",
    "    if user_data.shape[0]>1500:\n",
    "        # Get the basic information for the user\n",
    "        customer_information, registered_address = get_user_basic_info(user_data.iloc[0])\n",
    "        print(f\"Customer Information Loaded Successfully for {user}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now simialr to last approach, create a qdrant collection, embed the transaction data except customer information and registered address, and store the embeddings in the qdrant database and similarly create a test transaction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "# Initialize in-memory Qdrant client\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "# Create a collection in Qdrant for storing transaction embeddings\n",
    "client.create_collection(\n",
    "    collection_name=\"transactions\",\n",
    "    vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–‹         | 200/3085 [10:51<2:36:38,  3.26s/it]\n"
     ]
    }
   ],
   "source": [
    "for idx, (_, transaction) in tqdm(enumerate(user_data.iterrows()), total=len(user_data)):\n",
    "    # get the transactional information for a particular transaction\n",
    "    transaction_information, merchant_information, payment_address, merchant_address = get_transactional_data(transaction, convert_coordinates_to_address=True)\n",
    "    # convert the transaction information to string\n",
    "    transaction_description = convert_transaction_data_to_str(transaction_information, merchant_information, payment_address, merchant_address)\n",
    "    # embed the transaction description\n",
    "    embedding = embed_transaction(transaction_description, model, tokenizer)\n",
    "    embedding = embedding[0].tolist()\n",
    "    # upload the transaction embedding and data to the qdrant client\n",
    "    insert_transaction(embedding, transaction_description, idx, client)\n",
    "    time.sleep(1)\n",
    "\n",
    "    if idx == 200:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transaction_info = '''\n",
    "420000.54\n",
    "-----------------------\n",
    "Rajesh, Kumar; savings_account\n",
    "-----------------------\n",
    "Chandini Chowk; Delhi; India; 20.0583; 16.008\n",
    "-----------------------\n",
    "Vietnaam; 20.152538; 16.227746\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that everything is set up, let's move on to the next step where we will use the RAG model to explain the anomalies detected by the LLM model.\n",
    "\n",
    "RAG based approach starts by collecting the context for the LLM. To collect the context, we first embed the new transaction and then query the qdrant database to get the k closest transactions. We then extract the transaction description for these k closest transactions and store them in the context variable. This context will give the LLM an idea on how does the genuine transactions for this customer look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = get_context_for_anomaly_detection(new_transaction_info, client, model, tokenizer, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "96.56\n",
      "-----------------------\n",
      "Schumm, Bauch and Ondricka; grocery_pos\n",
      "-----------------------\n",
      "United States; Thomas; US; 26292; Tucker County; street; W; highway; trunk; Seneca Trail; West Virginia; 39.1505; -79.503\n",
      "-----------------------\n",
      "W; United States; highway; US; motorway; 26452; Senator Jennings Randolph Highway; Lewis County; West Virginia; street; 39.019265; -80.426668\n",
      "\n",
      "=============================NEW EXAMPLE===================================\n",
      "\n",
      "59.36\n",
      "-----------------------\n",
      "Goldner, Kovacek and Abbott; grocery_pos\n",
      "-----------------------\n",
      "United States; Thomas; US; 26292; Tucker County; street; W; highway; trunk; Seneca Trail; West Virginia; 39.1505; -79.503\n",
      "-----------------------\n",
      "W; United States; highway; US; service; Four M Road; Garrett County; Maryland; street; 39.448177; -79.2644\n",
      "\n",
      "=============================NEW EXAMPLE===================================\n",
      "\n",
      "90.3\n",
      "-----------------------\n",
      "Heller, Gutmann and Zieme; grocery_pos\n",
      "-----------------------\n",
      "United States; Thomas; US; 26292; Tucker County; street; W; highway; trunk; Seneca Trail; West Virginia; 39.1505; -79.503\n",
      "-----------------------\n",
      "W; United States; highway; US; residential; 26555; Dalewood Drive; Marion County; West Virginia; street; 39.495477; -80.215164\n",
      "\n",
      "=============================NEW EXAMPLE===================================\n",
      "\n",
      "8.54\n",
      "-----------------------\n",
      "Schumm PLC; shopping_net\n",
      "-----------------------\n",
      "United States; Thomas; US; 26292; Tucker County; street; W; highway; trunk; Seneca Trail; West Virginia; 39.1505; -79.503\n",
      "-----------------------\n",
      "United States; Somerset Township; US; 15330; Washington County; street; W; highway; Vanceville; unclassified; Hixon Road; Pennsylvania; 40.147271; -80.06872\n",
      "\n",
      "=============================NEW EXAMPLE===================================\n",
      "\n",
      "6.64\n",
      "-----------------------\n",
      "Yost, Block and Koepp; misc_pos\n",
      "-----------------------\n",
      "United States; Thomas; US; 26292; Tucker County; street; W; highway; trunk; Seneca Trail; West Virginia; 39.1505; -79.503\n",
      "-----------------------\n",
      "United States; Milford Township; US; 15557; Somerset County; street; W; highway; Gebhart; residential; Harvest Drive; Pennsylvania; 39.930006; -79.193979\n",
      "\n",
      "=============================NEW EXAMPLE===================================\n",
      "\n",
      "110.17\n",
      "-----------------------\n",
      "Koepp-Parker; grocery_pos\n",
      "-----------------------\n",
      "United States; Thomas; US; 26292; Tucker County; street; W; highway; trunk; Seneca Trail; West Virginia; 39.1505; -79.503\n",
      "-----------------------\n",
      "United States; Mount Solon; US; 22843; Augusta County; house; W; building; 1197; Mount Solon Road; Moscow; house; Virginia; 38.29637; -79.076893\n",
      "\n",
      "=============================NEW EXAMPLE===================================\n",
      "\n",
      "126.03\n",
      "-----------------------\n",
      "Kiehn-Emmerich; grocery_pos\n",
      "-----------------------\n",
      "United States; Thomas; US; 26292; Tucker County; street; W; highway; trunk; Seneca Trail; West Virginia; 39.1505; -79.503\n",
      "-----------------------\n",
      "United States; South Franklin Township; US; 15301; Washington County; street; W; highway; residential; Old Scales Road; Pennsylvania; 40.09147; -80.320551\n",
      "\n",
      "=============================NEW EXAMPLE===================================\n",
      "\n",
      "8.94\n",
      "-----------------------\n",
      "Osinski Inc; personal_care\n",
      "-----------------------\n",
      "United States; Thomas; US; 26292; Tucker County; street; W; highway; trunk; Seneca Trail; West Virginia; 39.1505; -79.503\n",
      "-----------------------\n",
      "W; United States; highway; US; residential; 21502; Our Lane; Allegany County; Maryland; street; 39.689658; -78.720249\n",
      "\n",
      "=============================NEW EXAMPLE===================================\n",
      "\n",
      "63.38\n",
      "-----------------------\n",
      "Tromp, Kerluke and Glover; grocery_net\n",
      "-----------------------\n",
      "United States; Thomas; US; 26292; Tucker County; street; W; highway; trunk; Seneca Trail; West Virginia; 39.1505; -79.503\n",
      "-----------------------\n",
      "39.959596000000005; -80.143364\n",
      "\n",
      "=============================NEW EXAMPLE===================================\n",
      "\n",
      "25.56\n",
      "-----------------------\n",
      "Beier-Hyatt; shopping_pos\n",
      "-----------------------\n",
      "United States; Thomas; US; 26292; Tucker County; street; W; highway; trunk; Seneca Trail; West Virginia; 39.1505; -79.503\n",
      "-----------------------\n",
      "38.405535; -80.102131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write some high quality system and user prompts to explain the LLM used in the RAG model about it's tasks and how it can help in detecting the anomalies and finding the reasons for the anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "You're an intelligent AI assistant that helps in detecting fraudulent transactions. \n",
    "\n",
    "You're provided with the three key information:\n",
    "    1. CUSTOMER INFORMATION: This has all the basic information about the customer which should give some idea about customer behaviour. The template is provided below.\n",
    "    2. CONTEXT: This has several  examples of a normal and non-fraudulent transactional information for the user. The template for each transaction is provided below.\n",
    "    3. NEW TRANSACTIONAL INFORMATION: This is the new transactional information that you need to classify as fraudulent or not. The template is same as normal transactional information \n",
    "\n",
    "Template for CUSTOMER INFORMATION and TRANSACTIONAL INFORMATION are provided below:\n",
    "    1. CUSTOMER INFORMATION TEMPLATE\n",
    "        {NAME}; {GENDER}; {AGE}; {JOB}\n",
    "        -----------------------\n",
    "        {REGISTERED ADDRESS}\n",
    "\n",
    "    2. TRANSACTIONAL INFORMATION TEMPLATE: \n",
    "        {AMOUNT}\n",
    "        -----------------------\n",
    "        {MERCHANT NAME}; {CATEGORY}\n",
    "        -----------------------\n",
    "        {PAYMENT ADDRESS}\n",
    "        -----------------------\n",
    "        {MERCHANT ADDRESS} \n",
    "\n",
    "Your task is to uderstand USER's personal information, registered address, and examples of normal transactional information based on template provided and classify the new transactional information as fraudulent or not based on the context provided and also provide the reason for your classification.\n",
    "\n",
    "You're only allowed to provide response in a json format with the following keys:\n",
    "    1. classification: This should be either of the following:\n",
    "        a. Fraudulent\n",
    "        b. Non-Fraudulent\n",
    "    2. reason: This should be a string explaining the reason for your classification.\n",
    "\n",
    "Example of the response:\n",
    "{\n",
    "    \"classification\": \"Fraudulent\",\n",
    "    \"reason\": \"The transaction amount is significantly higher than the average transaction amount.\"\n",
    "}\n",
    "    \n",
    "You can not provide any other response apart from the above mentioned json format with the keys mentioned above. In the classification key, you can only provide either \"Fraudulent\" or \"Non-Fraudulent\" as the value.\n",
    "'''\n",
    "\n",
    "prompt_template = f'''\n",
    "1. CUSTOMER INFORMATION:\n",
    "    {customer_information['name']}; {customer_information['gender']}; {customer_information['age']}; {customer_information['job']}\n",
    "    -----------------------\n",
    "    {registered_address['street']}; {registered_address['city']}; {registered_address['state']}; {registered_address['zip']}\n",
    "\n",
    "2. CONTEXT:\n",
    "    {context}\n",
    "\n",
    "3. NEW TRANSACTIONAL INFORMATION:\n",
    "    {new_transaction_info}\n",
    "\n",
    "RESPONSE:\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the quantized LLM model which will be used in our RAG based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00646209716796875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b9f503099f4cfbb4f105cc46324835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally pass the system prompts and user prompts based on the context retrieved from the qdrant database to the LLM model to identify the anomalies and explain the reasons for the anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": prompt_template},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classification': 'Fraudulent',\n",
       " 'reason': \"The transaction amount is significantly higher than the average transaction amount. The customer's registered address is in Thomas, WV, but the transaction is initiated from India, which is a different country and does not match the customer's registered address.\"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(outputs[0][\"generated_text\"][-1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we successfully have detected the anomalies and explained the reasons for the anomalies. You can make this approach more accurate by focusing on feature extraction and selection, prompt engineering, and threshold tuning. You can make this reverse recommendation architecture even more rhobust by having multiple vector stores for different types of transaction data and then do a granular analysis of the anomalies in each type of vector store before giving the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this blog we saw how we can combine the reverse recommendation system with the RAG model to not only detect the anomalies but also explain the reasons for the anomalies. This approach can be adapted to other use cases where we need to detect the anomalies from the data, such as insaurance claims, customer support interactions, monitoring the network traffic logs, etc. The key to this approach is to have a positive sample of the data and then use the test data to find out the deviations from the positive sample. If there's large deviation, then it's an anomaly and the RAG model can help in explaining the reasons for the anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
